\chapter{Discussion}
Biases on models from confounding elements have been widely reported in machine learning
\cite{DeConstructing_Bias_on_Skin_Lesion_Datasets_2019, Towards_Explainable_Classifiers_Using_the_Counterfactual_Approach_2019, debias-not-so-fast, interps-are-useful}.

The initial goal of this project was to look into methods to not use ruler presence in the
classification of lesions.
To be able to investigate the phenomenon, the plan was to:
\begin{enumerate}
    \item Train a model that performs fairly well compared to the state of the art \label{item:train-model}
    \item Show that the model is using the presence of ruler in its predictions \label{item:biased-ruler}
    \item Make changes to the model to remove the bias
    \item Rerun the argument as in step \ref{item:biased-ruler} and show that the model is no longer biased
\end{enumerate}
Step \ref{item:train-model} went fairly easy, especially because the dataset is well researched,
so another model could be used for inspiration \cite{kaggle-97-model}.

When reaching step \ref{item:biased-ruler}, the results didn't go quite as expected.
Even though it had been argued in previous papers, that models were doing exactly this
\cite{debias-not-so-fast,interps-are-useful}, I was unable to replicate their results,
to a degree that convinced me that the model was indeed using the rulers.
This changed the focus of the project from removing biased predictions to instead investigating
if there were any biases at all.
In the following, we will go through the experiments made in Section \ref{sec:testing-the-hypothesis} and
examine the results in this light: Do they seem to indicate that the model is using the rulers?

\section{Saliency maps}
In Section \ref{sec:prediction-saliency-map} we investigated the saliency maps of the model predictions.
Previously other researchers had shown that the rulers would be indicated clearly on these maps \cite{interps-are-useful}.
Going through the same experiment with the model trained in this project, we found saliency maps that were
did not indicate anything very clearly. 
(see Figure \ref{fig:interps-are-useful-saliency-maps} and \ref{fig:ruler_saliency_map}).
In some instances, the saliency map was not even visible in the hightligthed areas,
and in others it was but not in a way where it was obviously using that specific element of the image.

In general saliency maps are very prone to confirmation bias \cite{sanity-checks-for-saliency,Grns2020FaithfulSM}.
It is nothing more than the gradient on a very complicated function.
Conclude that just because the ruler is sometimes highlighted partly on the saliency map,
it must mean that the model is using it, seems like a stretch on its own.
Especially when considering the saliency maps from the model trained on segmented images (Figure 
\ref{fig:segmented_prediction_saliency_map}) that is hightlighting the ruler to at least the same degree 
that the model trained on the entire dataset is.
We know for sure, that the model trained on the segmented dataset is not using the rulers,
as it has never seen a ruler during training.
It is unclear exactly what knowledge can be extracted from the saliency maps,
but using them to conclude that the model is using the rulers doesn't seem to be holding up.

We could have used different kinds of saliency maps, if we wanted to go further into the understanding 
the model in this way.
Due to the risk of confirmation bias mentioned earlier, we instead used the time on other approaches.

\section{Feature vector similarity}
The authors of \cite{debias-not-so-fast} also argued that their model was using rulers
and other confounding elements in its predictions.
Their argument was based rougly around the idea that if the extracted features from two images
containing rulers are more similar to each other than the rest of the dataset, then the model is using them.

The first problem with using that approach in this report, is that the results didn't replicate.
Where the auther of \cite{debias-not-so-fast} showed that finding the nearest neighbors in the feature
vector space would all contain rulers, we only saw roughly half of them containing ruler 
(See their result on Figure \ref{fig:not-so-fast-artifact-query} and ours on Figure \ref{fig:my-artifact-query}).

Another problem, is that even if the results had contained more images with rulers,
it might not even have been the case that the model was even aware of them.
We have already shown, that the rulers correlate with the true diagnosis of the lesions (see Figure \ref{fig:ruler_vs_dx}).
Since the model is trained to predict the true diagnosis, we would expect them to have similar feature vectors.
What we see could therefore be the model finding other lesions that have similar diagnosis,
and therefore are also most likely to have rulers.

Even if images with rulers actually \textit{had} similar feature vectors \textit{because} of the rulers,
it would still not neccessarily be the case that the model was using them.
The architecture used in the model is pretrained on another big image dataset,
so a lot of the features that is extracted are not relevant to the final classification. 
The final fully connected layers of the model are in charge of extracting the relevant features.
So to use this method, we would also need to weight the features after how relevant they are to
the final classification.
That could for instance be done by running the backpropagation algorithm and see the
gradient in each of the features of the vector and weight by them in the distance metric.

\section{Statistical analysis of model predictions}
To investigate if the model was using the rulers in its predictions, 
we ran quite a few statistical tests (Section \ref{sec:statistical-tests}).
In most of the tests, we weren't able to identify that the model was biased.
In a few cases, we were able to identify statistically significant correlation between
the models prediction and the presence of rulers.
All of these related to the prediction strength of the model on images.
The points can be rougly summarized as:
\begin{enumerate}
    \item The model was worse at predicting the class of images that contained a ruler
    \item The model was worse at predicting the class of images of benign lesions where a ruler was present 
    \item The model was not significantly worse at predicting the class of malignant lesions if a ruler was present
\end{enumerate}

A tempting conclusion from a researcher looking into if a model is biased or not,
is that the model is classifying the benign lesions as malignant due to the ruler indicating them 
as malignant.
Another explanation could also be, that the rulers are simply present in benign images that are 
difficult to classify.
When a doctor decides to place a ruler next to a lesion,
it is due to a concern that the lesion is malignant.
If a benign lesion has a ruler, then it seems that the doctor found 
some aspect of the lesion that makes worried her, 
hence the lesion is probably more difficult to classify.

