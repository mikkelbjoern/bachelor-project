\chapter{Discussion}
Biases on models from confounding elements have been widely reported in machine learning
\cite{DeConstructing_Bias_on_Skin_Lesion_Datasets_2019, Towards_Explainable_Classifiers_Using_the_Counterfactual_Approach_2019, debias-not-so-fast, interps-are-useful}.

The initial goal of this project was to look into methods to not use ruler presence in the
classification of lesions.
To be able to investigate the phenomenon, the plan was to:
\begin{enumerate}
    \item Train a model that performs fairly well compared to the state of the art \label{item:train-model}
    \item Show that the model is using the presence of ruler in its predictions \label{item:biased-ruler}
    \item Make changes to the model to remove the bias
    \item Rerun the argument as in step \ref{item:biased-ruler} and show that the model is no longer biased
\end{enumerate}
Step \ref{item:train-model} went fairly easy, especially because the dataset is well researched,
so another model could be used for inspiration \cite{kaggle-97-model}.

When reaching step \ref{item:biased-ruler}, the results didn't go quite as expected.
Even though it had been argued in previous papers, that models were doing exactly this
\cite{debias-not-so-fast,interps-are-useful}, I was unable to replicate their results,
to a degree that convinced me that the model was indeed using the rulers.
This changed the focus of the project from removing biased predictions to instead investigating
if there were any biases at all.
In the following, we will go through the experiments made in Section \ref{sec:testing-the-hypothesis} and
examine the results in this light: Do they seem to indicate that the model is using the rulers?

\section{Saliency maps}
In Section \ref{sec:prediction-saliency-map} we investigated the saliency maps of the model predictions.
Previously other researchers had shown that the rulers would be indicated clearly on these maps \cite{interps-are-useful}.
Going through the same experiment with the model trained in this project, we found saliency maps that were
did not indicate anything very clearly. 
(see Figure \ref{fig:interps-are-useful-saliency-maps} and \ref{fig:ruler_saliency_map}).
In some instances, the saliency map was not even visible in the hightligthed areas,
and in others it was but not in a way where it was obviously using that specific element of the image.

In general saliency maps are very prone to confirmation bias \cite{sanity-checks-for-saliency,Grns2020FaithfulSM}.
It is nothing more than the gradient on a very complicated function.
Conclude that just because the ruler is sometimes highlighted partly on the saliency map,
it must mean that the model is using it, seems like a stretch on its own.
Especially when considering the saliency maps from the model trained on segmented images (Figure 
\ref{fig:segmented_prediction_saliency_map}) that is hightlighting the ruler to at least the same degree 
that the model trained on the entire dataset is.
We know for sure, that the model trained on the segmented dataset is not using the rulers,
as it has never seen a ruler during training.
It is unclear exactly what knowledge can be extracted from the saliency maps,
but using them to conclude that the model is using the rulers doesn't seem to be holding up.

We could have used different kinds of saliency maps, if we wanted to go further into the understanding 
the model in this way.
Due to the risk of confirmation bias mentioned earlier, we instead used the time on other approaches.
