\chapter{Testing the hypthesis}
From both the background litterature and general worry about models,
there is reason to believe that a model trained on the HAM10000 dataset will be biased
towards looking at confounding elements - hereunder the rulers in the image.
In the following tis hypothesis will be tested using both methods that other researchers have used,
and also other methods checking the output of the model.

\section{Prediction saliency map}\label{sec:prediction-saliency-map}
In \textit{Interpretations are useful: Penalizing Explanations to Align Neural Networks with Prior Knowledge}\cite{interps-are-useful},
(described in Section \ref{sec:interps-are-useful}), it is shown that a model trained on skin lesion images,
will mark out the ruler on malignant images containing one.
With the RESNET model described in \ref{sec:model}, similar saliency maps are constructed.

The saliency maps made here are gradient based as described in Section \ref{sec:gradiant_saliency_maps}.
In Figure \ref{fig:ruler_saliency_map} these can be seen.
More similar examples can be seen in Appendix \ref{appendix:ruler_saliency_maps}.

\begin{figure}[h]
    \includegraphics[
        width=\textwidth,
        height=\textheight,
        keepaspectratio=true,
        angle=0,
        clip=false
    ]{build/saliency_maps/overview_map_2.png}
    \caption{Saliency maps of the model prediction on an image with a ruler.}
    \label{fig:ruler_saliency_map}
\end{figure}

\section{Feature based nearest neighbors}
In \textit{Debiasing Skin Lesions Datasets and Model. Not so Fast}\cite{debias-not-so-fast} (described in Section \ref{sec:debias-not-so-fast}),
the authors argue that their melanoma prediction can pick up on confounding elements (like rulers), 
by examining the internal layers of their model and comparing them to one another.
Specifically, they find, that the vectors outputted from internal layers,
that are thought of as representing the semantic features of an image,
have a short euclidean distance to one another.
For a detailed description see section \ref{sec:debias-not-so-fast}.

To check if this is the case for our model, we use the same approach and make a similar plot as seen in Figure \ref{fig:not-so-fast-artifact-query}.

\begin{figure}
    \centering
    \includegraphics*[width=\textwidth]{build/near_neigh/examples.png}
    \caption{Queries like the ones in Figure \ref{fig:not-so-fast-artifact-query} made for my own model
            All queries contain a ruler, and the neighbors have noted if they contain a ruler.
    }
    \label{fig:my-artifact-query}
\end{figure}

Looking at Figure \ref{fig:my-artifact-query} we can see, that the results do not
suggest anywhere near as strong a correlation as the paper shows in Figure \ref{fig:not-so-fast-artifact-query}.
There does seem to be some amount more rulers than a random sample of images.
Weather that is neccessarily due to the model being able to detect the rulers will be discussed later.

These images don't show results nearly as clearly as the ones from the article.
Where the paper had results showing the ruler very clearly 0 these images will often just show a half of it.

\section{Statistical tests}
Since the tests described in the background litterature, don't show clear evidence,
that the model is biased towards looking at rulers, we will also do some basic statistical tests,
to see if the model output is statistically significantly impacted by rulers.
All tests will be performed under the assumption that the model is indeed impacted by ruler presence.

\subsection{Different performance on the melanoma class?}
The confusion matrix in Figure \ref{fig:prediction_strength} shows that the model underperforms on the \verb|mel| (melanoma) class.
Since the rulers are overly present in the images of lesions with melanoma,
it seems likely that the presence of the rulers improves the model's predictions 
on the \verb|mel| class.

To test this, a plot has been created below that shows the prediction strength of the model on the \verb|mel| class,
seperated over the presence of rulers (Figure \ref{fig:prediction_strength_mel}).

It shows a slightly better melanoma prediction precision on the pictures with rulers (Figure \ref{fig:prediction_strength_mel_normalized}).
Doing a $\chi^2$ test on the data from Figure \ref{fig:prediction_strength_mel_not_normalized},
however shows that the difference is not significant ($p=\input{build/prediction_strength/p_mel.txt}$).


\begin{figure}[h]
    \centering
    \begin{subfigure}[h]{0.45\textwidth}
        \includegraphics[
            width=\textwidth,
        ]{
            build/prediction_strength/mel_confusion_matrix_seaborn.png
        }
        \caption{No normalization}
        \label{fig:prediction_strength_mel_not_normalized}
    \end{subfigure}
    \begin{subfigure}[h]{0.45\textwidth}
        \includegraphics[
            width=\textwidth,
        ]{
            build/prediction_strength/mel_confusion_matrix_seaborn_normalized.png
        }
        \caption{Normalized over the presence of rulers}
        \label{fig:prediction_strength_mel_normalized}
    \end{subfigure}
    \caption{Confusion matrix of the model prediction the melanoma cases split up by presence of a ruler.}
    \label{fig:prediction_strength_mel}
\end{figure}

\subsection{Different performance on images containing rulers?}
If the rulers contributes to the model's predictions,
then it could be the case that the model has a general better/worse performance on images containing them.
To investigate this claim, a plot like the one in Figure \ref{fig:prediction_strength_mel} has been made,
where just Correct/Incorrect predictions are seperated over the presence of rulers.

\begin{figure}
    \centering
    \begin{subfigure}[h]{0.45\textwidth}
        \includegraphics[
            width=\textwidth,
        ]{
            build/prediction_strength/ruler_confusion_matrix_seaborn.png
        }
        \caption{No normalization}
        \label{fig:prediction_strength_ruler_not_normalized}
    \end{subfigure}
    \begin{subfigure}[h]{0.45\textwidth}
        \includegraphics[
            width=\textwidth,
        ]{
            build/prediction_strength/ruler_confusion_matrix_seaborn_normalized.png
        }
        \caption{Normalized over the presence of rulers}
        \label{fig:prediction_strength_ruler_normalized}
    \end{subfigure}
    \caption{Confusion matrix of the model prediction the cases split up by presence of a ruler.}
    \label{fig:prediction_strength_ruler}
\end{figure}

This plot tells a different story, then the one on just the \verb|mel| class.
The likelyhood of a falsely classified image is almost twice as high for images
that contain a ruler.
Doing a $\chi^2$ test on the data from the confusion matrix in Figure \ref{fig:prediction_strength_ruler_not_normalized},
shows a significant difference ($p=\input{build/prediction_strength/p_ruler.txt}$).