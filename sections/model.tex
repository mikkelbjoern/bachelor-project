\chapter{Model}\label{sec:model}
The initial goal of this project was to look into methods to not use ruler presence in the
diagnosis classification.
To be able to do this, a model that was using the ruler presence in its predictions was needed.
Preferably this model should also perform close to the best performing model, to be comparable.

\section{ResNet18 architecture trained with MixUp}
On the Kaggle classification competetion related to the HAM10000 dataset\cite{HAM10000-kaggle-competetion},
another user claimed to get a $97.7\%$ accuracy using a ResNet18 model trained with MixUp\cite{kaggle-97-model}.
When the provided model was retrained, the results were not as good as claimed.
The actual accuracy was in the range of $90\%$ to $92\%$, depending on the data split.
That is however still close to the best performing model published (see \ref{sec:state-of-the-art}).

The actual implementation of the model can be seen in Appendix \ref{appendix:resnet-18-mixup}.
Using the slight modifications to the original mode, the model reached an accuracy of roughly $91\%$.

\section{Prediction precision on different classes}
The model is trained on roughly $80\%$ of the data, and is tested on the remaining $20\%$.
In Figure \ref{fig:prediction_strength} a confusion matrix for the trained model tested on the test set is shown.

\begin{figure}[ht]
    \centering
    \includegraphics[
        width=0.7\textwidth,
    ]{build/prediction_strength/confusion_matrix_seaborn.png}
    \caption{Confusion matrix of the model prediction on the dataset. 
        Normalization has been done over the truth.
    }
    \label{fig:prediction_strength}
\end{figure}